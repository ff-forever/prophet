{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, The cost function in logistic regression, also known as the logistic loss or cross-entropy loss, measures how well the logistic regression model's predictions match the actual labels of the training data. The goal of training a logistic regression model is to find the parameters (weights) that minimize this cost function.\n",
    "\n",
    "For a single training pair:\n",
    "$\\hat{y} = P(Y=1|X=x) = \\sigma(wÂ·x + b)$,  where  $\\sigma(z) = \\frac{1}{1+e^{z}} $\n",
    "\n",
    "$P(y|x) = \\begin{cases}\\hat{y} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ if\\ y = 1\\\\ (1-\\hat{y})\\ \\ \\ if\\ y = 0 \\end{cases} $\n",
    "\n",
    "If the labels are 0 and 1, then Y is a Bernoulli random variable Y âˆ¼ Ber(p), where p = $\\sigma(wÂ·x + b)$\n",
    "\n",
    "so,P(y|x) = $\\hat{y}^{y}(1-\\hat{y})^{1-y} $, Taking the log of both sides without changing their monotonicity yields:\n",
    "\n",
    "$logP(y|x) = ylog(\\hat{y}) + (1-y)log(1-\\hat{y}) = -L(\\hat{y},y) $\n",
    "\n",
    "The cost function for logistic regression is derived from the likelihood of the observed data. The likelihood is the probability of observing the given set of labels $y_{1},y_{2},y_{3}...y_{n} $ for the given set of inputs $x_{1},x_{2},x_{3}...x_{n} $ given the model parameters ğ‘¤ and ğ‘, To derive the cost function, we sum the negative log-likelihoods over all training examples and then take the averageï¼š\n",
    "\n",
    "$$ J(w,b) = -log \\prod_{i=1}^{n}p(y^{[i]}|x^{[i]}) = - \\frac{1}{n}\\sum_{i=1}^{n}\\left(y^{[i]}log(\\hat{y}^{[i]}) + (1-y^{[i]})log(1-\\hat{y}^{[i]})\\right) $$\n",
    "\n",
    "Putting it all together, the cost function J(w,b) for logistic regression is:\n",
    "$$ J(w,b) = - \\frac{1}{i}\\sum_{n=1}^{n}\\left(y^{[i]}log(\\sigma(w^{T}Â·x^{[i]} + b)) + (1-y^{[i]})log(1-\\sigma(w^{T}Â·x^{[i]} + b))\\right) $$\n",
    "\n",
    "where: \n",
    "\n",
    "i is the number of training examples.\n",
    "\n",
    "$ y^{[i]} $ is the actual label for the i-th training example\n",
    "\n",
    "$\\sigma(w^{T}Â·x^{[i]} + b)$ is the predicted probability for the i-th training example\n",
    "\n",
    "\n",
    "when $ y^{[i]} = 1 $, the term $ (1-y^{[i]})log(1-\\hat{y}^{[i]}) $ drops out, and the cost function focuses on $ log(\\hat{y}^{[i]}) $,If the model predicts a low probability for $ y^{[i]} = 1 $\n",
    ", the cost will be high, encouraging the model to increase $\\hat{y}^{[i]}$\n",
    "\n",
    "when $ y^{[i]} = 0 $, the term $ y^{[i]}log\\hat{y}^{[i]} $ drops out, and the cost function focuses on $1 - log(\\hat{y}^{[i]}) $,If the model predicts a low probability for $ y^{[i]} = 0 $\n",
    ", the cost will be high, encouraging the model to decrease $\\hat{y}^{[i]}$\n",
    "\n",
    "By minimizing this cost function using optimization techniques like gradient descent, the logistic regression model learns the optimal parameters w and b that best fit the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Voting classifiers are a type of ensemble learning method used to improve the performance and robustness of predictive models by combining the predictions of multiple base models (classifiers). The main idea behind voting classifiers is that by aggregating the predictions of several models, the overall prediction is more accurate and less likely to be influenced by the weaknesses of individual models, There are two main types of voting methods in ensemble learning: hard voting and soft voting\n",
    "\n",
    "In hard voting, also known as majority voting, each base classifier makes a prediction (a class label). The final prediction of the ensemble is the class label that receives the majority of votes from the base classifiers\n",
    "\n",
    "In soft voting, each base classifier outputs a probability for each class. The final prediction is made by averaging the predicted probabilities and selecting the class with the highest average probability. This method takes into account the confidence of each classifier's predictions\n",
    "\n",
    "The benefits of Voting Classifiers is: Improved Accuracy,Reduced Overfitting,Robustness and Versatility; The limitations is: Complexity,Computational Cost and Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the seven-step method to build a model to predict the rise and fall of the Shanghai Composite Index \n",
    "1, Data Collection\n",
    "é€šè¿‡é›…è™é‡‘èä¸‹è½½ä¸Šè¯æŒ‡æ•°2016/1/1åˆ°2024/6/3çš„äº¤æ˜“æ•°æ®\n",
    "2, Data Preprocessing\n",
    "ä½¿ç”¨å‡½æ•°StandardScalerå¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†å¤„ç†\n",
    "3,Feature Engineering\n",
    "é€‰æ‹©features\n",
    "'HC', 'Sign', 'RET', 'VMA_5', 'VMA_10', 'VMA_20', 'VMA_60', 'VMA_120',\n",
    "'OC', 'OC7', 'OC14', 'HL', 'HC7', 'HC4', 'STD', 'SMA_5', 'SMA_10',\n",
    "'SMA_20', 'SMA_60', 'SMA_120', 'EMA_5', 'EMA_10', 'EMA_20', 'EMA_60',\n",
    "'EMA_120', 'Momentum__5', 'Momentum__10', 'Momentum__20',\n",
    "'Momentum__60', 'Momentum__120'\n",
    "å¯¹featureè¿›è¡Œä¼˜åŒ–ï¼Œå»æ‰ç›¸å…³æ€§å¤§äº0.9çš„featureå¾—åˆ°å¦‚ä¸‹feature\n",
    "'HC', 'Sign', 'RET', 'VMA_5', 'VMA_10', 'VMA_60', 'OC', 'OC7', 'OC14',\n",
    "'HL', 'HC7', 'STD', 'SMA_5', 'Momentum__5', 'Momentum__10',\n",
    "'Momentum__20', 'Momentum__60', 'Momentum__120'\n",
    "Model Selection\n",
    "leabel ä½¿ç”¨ä½¿ç”¨SVMåˆ†ç±»å™¨å¯¹è¿›è¡Œfeatureè¿›è¡Œæ‹Ÿåˆ\n",
    "Model Training\n",
    "ç”¨80%çš„æ•°æ®è¿›è¡Œè®­ç»ƒ20%çš„æ•°æ®è¿›è¡Œè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ï¼ŒModel Evaluation\n",
    "The results of using the svc model without parameter adjustment are Train Accuracy: 0.6708, Test Accuracy: 0.6528. It can be seen that the accuracy of both training data and test data is not very good. It can be seen that the prediction of the decline is very accurate, while the prediction of the rise is very poor. Overall, the prediction is not optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/Confusion matrix.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the AUC-ROC curve, we can see that the AUC value is 0.5, which is basically random, a little higher than random guessing, and the model is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/AUC-ROC Curve.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹å‚æ•°tolï¼ŒCè¿›è¡Œæœ€ä¼˜åŒ–è°ƒèŠ‚ï¼Œå¾—åˆ°'tol': 0.009878566632491585, 'C': 9.930264730821525ï¼Œä½¿ç”¨è°ƒèŠ‚åçš„å‚æ•°è¿›è¡Œæµ‹è¯•å¾—åˆ° Training Accuracy:0.8177ï¼ŒTest Accuracy:0.614ï¼Œå¯ä»¥çœ‹åˆ°å‚æ•°è°ƒèŠ‚ä¹‹åï¼Œè®©è®­ç»ƒå‡†ç¡®ç‡å˜çš„æ›´é«˜ï¼Œè€Œæµ‹è¯•çš„å‡†ç¡®ç‡å´å˜ä½äº†ï¼Œé€šè¿‡è°ƒå‚åçš„æ··æ·†çŸ©é˜µå¯ä»¥çœ‹å‡ºå‚æ•°è°ƒèŠ‚åä¹Ÿæœ‰äº›æ•ˆæœå°±æ˜¯å¯¹ä¸Šæ¶¨çš„é¢„æµ‹æ¦‚ç‡é«˜äº†ç‚¹ï¼Œä½†è¿˜æ˜¯é¢„æµ‹æ­£ç¡®ç‡è¿˜æ˜¯å¾ˆä½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/Confusion matrix_2.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./image/AUC-ROC Curve_2.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
