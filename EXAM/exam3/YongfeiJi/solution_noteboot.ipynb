{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, The cost function in logistic regression, also known as the logistic loss or cross-entropy loss, measures how well the logistic regression model's predictions match the actual labels of the training data. The goal of training a logistic regression model is to find the parameters (weights) that minimize this cost function.\n",
    "\n",
    "For a single training pair:\n",
    "$\\hat{y} = P(Y=1|X=x) = \\sigma(w·x + b)$,  where  $\\sigma(z) = \\frac{1}{1+e^{z}} $\n",
    "\n",
    "$P(y|x) = \\begin{cases}\\hat{y} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ if\\ y = 1\\\\ (1-\\hat{y})\\ \\ \\ if\\ y = 0 \\end{cases} $\n",
    "\n",
    "If the labels are 0 and 1, then Y is a Bernoulli random variable Y ∼ Ber(p), where p = $\\sigma(w·x + b)$\n",
    "\n",
    "so,P(y|x) = $\\hat{y}^{y}(1-\\hat{y})^{1-y} $, Taking the log of both sides without changing their monotonicity yields:\n",
    "\n",
    "$logP(y|x) = ylog(\\hat{y}) + (1-y)log(1-\\hat{y}) = -L(\\hat{y},y) $\n",
    "\n",
    "The cost function for logistic regression is derived from the likelihood of the observed data. The likelihood is the probability of observing the given set of labels $y_{1},y_{2},y_{3}...y_{n} $ for the given set of inputs $x_{1},x_{2},x_{3}...x_{n} $ given the model parameters 𝑤 and 𝑏, To derive the cost function, we sum the negative log-likelihoods over all training examples and then take the average：\n",
    "\n",
    "$$ J(w,b) = -log \\prod_{i=1}^{n}p(y^{[i]}|x^{[i]}) = - \\frac{1}{n}\\sum_{i=1}^{n}\\left(y^{[i]}log(\\hat{y}^{[i]}) + (1-y^{[i]})log(1-\\hat{y}^{[i]})\\right) $$\n",
    "\n",
    "Putting it all together, the cost function J(w,b) for logistic regression is:\n",
    "$$ J(w,b) = - \\frac{1}{i}\\sum_{n=1}^{n}\\left(y^{[i]}log(\\sigma(w^{T}·x^{[i]} + b)) + (1-y^{[i]})log(1-\\sigma(w^{T}·x^{[i]} + b))\\right) $$\n",
    "\n",
    "where: \n",
    "\n",
    "i is the number of training examples.\n",
    "\n",
    "$ y^{[i]} $ is the actual label for the i-th training example\n",
    "\n",
    "$\\sigma(w^{T}·x^{[i]} + b)$ is the predicted probability for the i-th training example\n",
    "\n",
    "\n",
    "when $ y^{[i]} = 1 $, the term $ (1-y^{[i]})log(1-\\hat{y}^{[i]}) $ drops out, and the cost function focuses on $ log(\\hat{y}^{[i]}) $,If the model predicts a low probability for $ y^{[i]} = 1 $\n",
    ", the cost will be high, encouraging the model to increase $\\hat{y}^{[i]}$\n",
    "\n",
    "when $ y^{[i]} = 0 $, the term $ y^{[i]}log\\hat{y}^{[i]} $ drops out, and the cost function focuses on $1 - log(\\hat{y}^{[i]}) $,If the model predicts a low probability for $ y^{[i]} = 0 $\n",
    ", the cost will be high, encouraging the model to decrease $\\hat{y}^{[i]}$\n",
    "\n",
    "By minimizing this cost function using optimization techniques like gradient descent, the logistic regression model learns the optimal parameters w and b that best fit the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Voting classifiers are a type of ensemble learning method used to improve the performance and robustness of predictive models by combining the predictions of multiple base models (classifiers). The main idea behind voting classifiers is that by aggregating the predictions of several models, the overall prediction is more accurate and less likely to be influenced by the weaknesses of individual models, There are two main types of voting methods in ensemble learning: hard voting and soft voting\n",
    "\n",
    "In hard voting, also known as majority voting, each base classifier makes a prediction (a class label). The final prediction of the ensemble is the class label that receives the majority of votes from the base classifiers\n",
    "\n",
    "In soft voting, each base classifier outputs a probability for each class. The final prediction is made by averaging the predicted probabilities and selecting the class with the highest average probability. This method takes into account the confidence of each classifier's predictions\n",
    "\n",
    "The benefits of Voting Classifiers is: Improved Accuracy,Reduced Overfitting,Robustness and Versatility; The limitations is: Complexity,Computational Cost and Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the seven-step method to build a model to predict the rise and fall of the Shanghai Composite Index \n",
    "1, Data Collection\n",
    "通过雅虎金融下载上证指数2016/1/1到2024/6/3的交易数据\n",
    "2, Data Preprocessing\n",
    "使用函数StandardScaler对数据进行标准化处理处理\n",
    "3,Feature Engineering\n",
    "选择features\n",
    "'HC', 'Sign', 'RET', 'VMA_5', 'VMA_10', 'VMA_20', 'VMA_60', 'VMA_120',\n",
    "'OC', 'OC7', 'OC14', 'HL', 'HC7', 'HC4', 'STD', 'SMA_5', 'SMA_10',\n",
    "'SMA_20', 'SMA_60', 'SMA_120', 'EMA_5', 'EMA_10', 'EMA_20', 'EMA_60',\n",
    "'EMA_120', 'Momentum__5', 'Momentum__10', 'Momentum__20',\n",
    "'Momentum__60', 'Momentum__120'\n",
    "对feature进行优化，去掉相关性大于0.9的feature得到如下feature\n",
    "'HC', 'Sign', 'RET', 'VMA_5', 'VMA_10', 'VMA_60', 'OC', 'OC7', 'OC14',\n",
    "'HL', 'HC7', 'STD', 'SMA_5', 'Momentum__5', 'Momentum__10',\n",
    "'Momentum__20', 'Momentum__60', 'Momentum__120'\n",
    "Model Selection\n",
    "leabel 使用使用SVM分类器对进行feature进行拟合\n",
    "Model Training\n",
    "用80%的数据进行训练20%的数据进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6，Model Evaluation\n",
    "The results of using the svc model without parameter adjustment are Train Accuracy: 0.6708, Test Accuracy: 0.6528. It can be seen that the accuracy of both training data and test data is not very good. It can be seen that the prediction of the decline is very accurate, while the prediction of the rise is very poor. Overall, the prediction is not optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/Confusion matrix.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the AUC-ROC curve, we can see that the AUC value is 0.5, which is basically random, a little higher than random guessing, and the model is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/AUC-ROC Curve.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对参数tol，C进行最优化调节，得到'tol': 0.009878566632491585, 'C': 9.930264730821525，使用调节后的参数进行测试得到 Training Accuracy:0.8177，Test Accuracy:0.614，可以看到参数调节之后，让训练准确率变的更高，而测试的准确率却变低了，通过调参后的混淆矩阵可以看出参数调节后也有些效果就是对上涨的预测概率高了点，但还是预测正确率还是很低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/Confusion matrix_2.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./image/AUC-ROC Curve_2.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
